{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data.csv')\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:,:])\n",
    "X[:,:] = imputer.transform(X[:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the independent variable (considering multicollinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_prev (l_in):\n",
    "    l_out = []\n",
    "    l_out.append(l_in[0])\n",
    "    for i in range(len(l_in)-1):\n",
    "        l_out.append(l_out[i] + l_in[i+1])\n",
    "    return [e - 1 for e in l_out]\n",
    "\n",
    "# df and X must have the same data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "columns_to_encode = [0, 2, 3] # Change here\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), columns_to_encode)], remainder='passthrough')\n",
    "columns_to_encode = [df.iloc[:, del_idx].nunique() for del_idx in columns_to_encode]\n",
    "columns_to_encode = sum_prev(columns_to_encode)\n",
    "X = np.array(ct.fit_transform(X))\n",
    "X = np.delete(X, columns_to_encode, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the independent variable (not considering multicollinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [column_index_to_encode])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling and undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "sm = SMOTE()\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss_X = StandardScaler()\n",
    "X_train = ss_X.fit_transform(X_train)\n",
    "X_test = ss_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "ss_y = StandardScaler()\n",
    "y_train = ss_y.fit_transform(y_train)\n",
    "y_test = ss_y.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning to original value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_X.inverse_transform(X_train)\n",
    "ss_X.inverse_transform(X_test)\n",
    "ss_y.inverse_transform(y_train)\n",
    "ss_y.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'param_1': [arg_1, arg_2], 'param_2': [arg_1, arg_2]}\n",
    "grid = GridSearchCV(ml_model(), param_grid, cv=num_for_k_folds, verbose=2)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_params_\n",
    "grid.best_estimator_\n",
    "grid_predictions = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict([[x1_value, x2_value]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing predicted values and actual values side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using test set (with feature scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ss_y.inverse_transform(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a value (with feature scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(ss_X.transform([[x1_value, x2_value]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing predicted values and actual values side-by-side (with feature scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = ss_y.inverse_transform(y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics: https://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.r2_score(y_test, y_pred))\n",
    "print(metrics.mean_absolute_error(y_test, y_pred))\n",
    "print(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot((y_test-y_pred)) # If it's a correct model choice, it should be normally distributed\n",
    "plt.xlabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the actual and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_test, y=y_pred) # If the model fitted well, it should be a straight line\n",
    "plt.xlabel('y_test')\n",
    "plt.ylabel('y_pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n",
    "print('Confusion matrix:\\n', metrics.confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy:', metrics.accuracy_score(y_test, y_pred))\n",
    "print('Precision:', metrics.precision_score(y_test, y_pred))\n",
    "print('Recall:', metrics.recall_score(y_test, y_pred))\n",
    "print('F1-Score:', metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred)) # Better for multiclass problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the testing set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test, y_test, color='red') # Use train for training set\n",
    "plt.plot(X_train, regression.predict(X_train), color='blue')\n",
    "plt.title('y vs x (Test set)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the test set results (higher resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid_train = np.arange(min(X), max(X), 0.1)\n",
    "X_grid_train = X_grid_train.reshape(len(X_grid), 1)\n",
    "plt.scatter(X_test, y_test, color='red') # Use train for training set\n",
    "plt.plot(X_grid_train, regression.predict(X_grid_train), color='blue')\n",
    "plt.title('y vs x (Test set)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the test set results (higher resolution, with feature scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid_train = np.arange(min(X), max(X), 0.1)\n",
    "X_grid_train = X_grid_train.reshape(len(X_grid), 1)\n",
    "plt.scatter(ss_X.inverse_transform(X_test), ss_y.inverse_transform(y_test), color='red') # Use train for training set\n",
    "plt.plot(ss_X.inverse_transform(X_grid_train), ss_y.inverse_transfor(regression.predict(X_grid_train)), color='blue')\n",
    "plt.title('y vs x (Test set)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the testing set results (higher resolution, slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = ss_X.inverse_transform(X_test), y_test # X_train, y_train for training set\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(ss_X.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('y vs x (Test set)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the training set results (lower resolution, faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_train, y_train # X_train, y_train for training set\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('y vs x (Test set)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regression.coef_)\n",
    "print(regression.intercept_)\n",
    "pd.DataFrame(regressor.coef_, X_train.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regression.coef_)\n",
    "print(regression.intercept_)\n",
    "pd.DataFrame(regression.coef_, df.columns[:-1], columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Linear Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree = number_of_polynomials)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regression.predict(poly_features.fit_transform([[x1_value, x2_value]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting linear equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regression.coef_)\n",
    "print(regression.intercept_)\n",
    "pd.DataFrame(regression.coef_, X_train.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the test set results (higher resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid_train = np.arange(min(X), max(X), 0.1)\n",
    "X_grid_train = X_grid_train.reshape(len(X_grid_train), 1)\n",
    "plt.scatter(X_test, y_test, color='red')\n",
    "plt.plot(X_grid_train, regression.predict(poly_reg.fit_transform(X_grid_train)), color='blue')\n",
    "plt.title('y vs x (Test set)')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score \n",
    "degrees = [2, 3, 4, 5, 6] # Change degree \"hyperparameter\" here\n",
    "normalizes = [True, False] # Change normalize hyperparameter here\n",
    "best_score = 0\n",
    "best_degree = 0\n",
    "for degree in degrees:\n",
    "    for normalize in normalizes:\n",
    "        poly_features = PolynomialFeatures(degree = degree)\n",
    "        X_train_poly = poly_features.fit_transform(X_train)\n",
    "        polynomial_regression = LinearRegression(normalize=normalize)\n",
    "        polynomial_regression.fit(X_train_poly, y_train)\n",
    "        scores = cross_val_score(polynomial_regression, X_train_poly, y_train, cv=5) # Change k-fold cv value here\n",
    "        if max(scores) > best_score:\n",
    "            best_score = max(scores)\n",
    "            best_degree = degree\n",
    "            best_normalize = normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "\n",
    "Needs feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "regressor = SVR(kernel='kernal_name')\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "     'kernel': ['rbf', 'sigmoid']},\n",
    "    {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "     'kernel': ['linear']},\n",
    "    {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "     'degree': [2, 3, 4, 5, 6],\n",
    "     'kernel': ['poly']}\n",
    "]\n",
    "grid = GridSearchCV(SVC(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'C': [0.1, 1, 10, 100, 1000], \n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "     'kernel': ['rbf']},\n",
    "    {'C': [0.1, 1, 10, 100, 1000],\n",
    "     'kernel': ['linear']},\n",
    "    {'C': [0.1, 1, 10, 100, 1000],\n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "     'degree': [2, 3],\n",
    "     'kernel': ['poly']}\n",
    "]\n",
    "grid = GridSearchCV(SVC(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Non-continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor()\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['mse', 'friedman_mse', 'mae'],\n",
    "    'max_depth': [None, 5, 10, 20, 30, 40, 50, 80, 90, 100, 110],\n",
    "    'max_features': [2, 3, 5, 10, 'auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 10, 15, 100],\n",
    "    'min_samples_split': [2, 5, 8, 10, 12, 15, 20],\n",
    "    'n_estimators': [100, 200, 300, 500, 800, 1000, 1500, 2500]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestRegressor(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [10, 40, 70, 100],\n",
    "    'max_features': [2, 3, 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 3, 6, 10, 14],\n",
    "    'min_samples_split': [2, 6, 10, 14],\n",
    "    'n_estimators': [100, 300]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestRegressor(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Non-continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=200)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [None, 5, 10, 20, 30, 40, 50, 80, 90, 100, 110],\n",
    "    'max_features': [2, 3, 5, 10, 'auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 10, 15, 100],\n",
    "    'min_samples_split': [2, 5, 8, 10, 12, 15, 20],\n",
    "    'n_estimators': [100, 200, 300, 500, 800, 1000, 1500, 2500]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestRegressor(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [10, 40, 70, 100],\n",
    "    'max_features': [2, 3, 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 3, 6, 10, 14],\n",
    "    'min_samples_split': [2, 6, 10, 14],\n",
    "    'n_estimators': [100, 300]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestRegressor(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Needs feature scaling\n",
    "\n",
    "Linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'penalty': ['none', '11', '12', 'elasticnet'], \n",
    "    'C': list(np.logspace(0, 4, 10)) + [0.0001, 0.01, 0.05, 0.2, 10, 1000],\n",
    "    'solver': [\"newton-cg\", \"lbfgs\", \"liblinear\", 'sag', 'saga'],\n",
    "    'dual': [True, False]\n",
    "}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'penalty': ['11', '12'], \n",
    "    'C': [0.001, 0.01, 1, 10, 100],\n",
    "    'solver': [\"newton-cg\", \"lbfgs\"]\n",
    "}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "    \n",
    "Needs feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method to find optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "error_rate = []\n",
    "\n",
    "for i in range(1, 40, 2):\n",
    "    knn_elbow = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_elbow.fit(X_train, y_train)\n",
    "    y_pred = knn_elbow.predict(X_test)\n",
    "    error_rate.append(1 - accuracy_score(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1, 40, 2), error_rate, color='blue', ls='--', marker='o', markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'n_neighbors': list(range(1, 31, 2)),\n",
    "     'weights': ['uniform', 'distance'],\n",
    "     'metric': ['euclidean', 'manhattan', 'minkowski']},\n",
    "    {'n_neighbors': list(range(1, 31, 2)),\n",
    "     'weights': ['uniform', 'distance'],\n",
    "     'p': [3, 4, 5]\n",
    "     'metric': ['minkowski']}\n",
    "]\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 25, 2)),\n",
    "    'p': [1, 2],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    \n",
    "Needs feature scaling\n",
    "\n",
    "Non-linear (unless if using the linear kernel)\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# The larger the C value the more the model will overfit\n",
    "# The larger the gamma value the more the model will underfit\n",
    "classifier = SVC(kernel='rbf')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "     'kernel': ['rbf', 'sigmoid']},\n",
    "    {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "     'kernel': ['linear']},\n",
    "    {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "     'degree': [2, 3, 4, 5, 6],\n",
    "     'kernel': ['poly']}\n",
    "]\n",
    "grid = GridSearchCV(SVC(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'C': [ 0.1, 1, 10, 100, 1000], \n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "     'kernel': ['rbf']},\n",
    "    {'C': [0.1, 1, 10, 100, 1000],\n",
    "     'kernel': ['linear']},\n",
    "    {'C': [0.1, 1, 10, 100, 1000],\n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "     'degree': [2, 3],\n",
    "     'kernel': ['poly']}\n",
    "]\n",
    "grid = GridSearchCV(SVC(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.htmlhtml\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Non-continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10, 20, 30, 40, 50, 80, 90, 100, 110],\n",
    "    'max_features': [2, 3, 5, 10, 'auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 10, 15, 100],\n",
    "    'min_samples_split': [2, 5, 8, 10, 12, 15, 20],\n",
    "    'n_estimators': [100, 200, 300, 500, 800, 1000, 1500, 2500]\n",
    "}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [10, 40, 70, 100],\n",
    "    'max_features': [2, 3, 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 3, 6, 10, 14],\n",
    "    'min_samples_split': [2, 6, 10, 14],\n",
    "    'n_estimators': [100, 300]\n",
    "}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Non-continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regressor = RandomForestClassifier(n_estimators=200)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [None, 5, 10, 20, 30, 40, 50, 80, 90, 100, 110],\n",
    "    'max_features': [2, 3, 5, 10, 'auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 10, 15, 100],\n",
    "    'min_samples_split': [2, 5, 8, 10, 12, 15, 20],\n",
    "    'n_estimators': [100, 200, 300, 500, 800, 1000, 1500, 2500]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [10, 40, 70, 100],\n",
    "    'max_features': [2, 3, 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 3, 6, 10, 14],\n",
    "    'min_samples_split': [2, 6, 10, 14],\n",
    "    'n_estimators': [100, 300]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNBm\n",
    "\n",
    "Needs feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Continuous\n",
    "\n",
    "No hyperparameters to tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 1.0]\n",
    "}\n",
    "grid = GridSearchCV(MultinomialNB(), param_grid, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "Needs feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method to find optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "\n",
    "for k in range(2, 21):\n",
    "    kmeans_elbow = KMeans(n_clusters=k)\n",
    "    kmeans_elbow.fit(X)\n",
    "    wcss.append(kmeans_elbow.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(2, 21), wcss, ls='--', marker='o', markerfacecolor='red', markersize=10)\n",
    "plt.title('WCSS vs k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('WCSS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=8)\n",
    "kmeans.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'blue', 'green', 'cyan', 'magenta']\n",
    "\n",
    "for i in range(kmeans.n_clusters):\n",
    "    plt.scatter(X[kmeans.labels_ == i, 0], X[kmeans.labels_ == i, 1], c=colors[i], s=50, label='Cluster ' + str(i))\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=150, c='black', label='Centroids')  \n",
    "plt.title('Clusters')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering\n",
    "\n",
    "Needs feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendogram to find optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "dendogram = sch.dendrogram(sch.linkage(X, metric='euclidean', method='ward'), no_labels=True)\n",
    "plt.title('Dendogram')\n",
    "plt.xlabel('Rows')\n",
    "plt.ylabel('Euclidean distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "y_pred = hc.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'blue', 'green', 'cyan', 'magenta']\n",
    "\n",
    "for i in range(hc.n_clusters):\n",
    "    plt.scatter(X[hc.labels_ == i, 0], X[hc.labels_ == i, 1], c=colors[i], s=50, label='Cluster ' + str(i))\n",
    "\n",
    "plt.title('Clusters')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "N = df.shape[0]\n",
    "d = df.shape[1]\n",
    "options_selected = []\n",
    "numbers_of_selections = [1] * d\n",
    "sums_of_rewards = [0] * d\n",
    "total_reward = 0\n",
    "\n",
    "for n in range(N):\n",
    "    option = 0\n",
    "    max_upper_bound = 0\n",
    "    for i in range(d):\n",
    "        average_reward = sums_of_rewards[i] / numbers_of_selections[i]\n",
    "        delta_i = math.sqrt(3/2 * math.log(n+1) / numbers_of_selections[i])\n",
    "        upper_bound = average_reward + delta_i\n",
    "        if upper_bound > max_upper_bound:\n",
    "            max_upper_bound = upper_bound\n",
    "            option = i\n",
    "    options_selected.append(option)\n",
    "    numbers_of_selections[option] += 1\n",
    "    reward = df.iloc[n, option]\n",
    "    sums_of_rewards[option] += reward\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the options selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(options_selected)\n",
    "plt.title('Histogram of options selections')\n",
    "plt.xlabel('Options')\n",
    "plt.ylabel('Number of times each option was selected by UCB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N = df.shape[0]\n",
    "d = df.shape[1]\n",
    "options_selected = []\n",
    "number_of_rewards_1 = [0] * d\n",
    "number_of_rewards_0 = [0] * d\n",
    "total_reward = 0\n",
    "\n",
    "for n in range(N):\n",
    "    option = 0\n",
    "    max_random_beta = 0\n",
    "    for i in range(d):\n",
    "        random_beta = random.betavariate(number_of_rewards_1[i] + 1, number_of_rewards_0[i] + 1)\n",
    "        if random_beta > max_random_beta:\n",
    "            max_random_beta = random_beta\n",
    "            option = i\n",
    "    options_selected.append(option)\n",
    "    reward = df.iloc[n, option]\n",
    "    if reward == 1:\n",
    "        number_of_rewards_1[option] += 1\n",
    "    else:\n",
    "        number_of_rewards_0[option] += 1\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the options selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(options_selected)\n",
    "plt.title('Histogram of ads selections')\n",
    "plt.xlabel('Ads')\n",
    "plt.ylabel('Number of times each ad was selected by the algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rule Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/apyori/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising items' frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stack().value_counts(normalize=True)[:10].plot(kind='bar', title='Relative Frequency')\n",
    "df.stack().value_counts().apply(lambda item: item / df.shape[0])[:10].plot(kind='bar', title='Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the dataset (models's input is a list of lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = []\n",
    "for i in range(df.shape[0]):\n",
    "    row = df.iloc[i].dropna().tolist()\n",
    "    transactions.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "rules = apriori(transactions=transactions, \n",
    "                min_support=0.2, # how frequent is your item(s) in the dataset \n",
    "                min_confidence=0.6, # how often your rule will work\n",
    "                min_lift=2, # how better off you are compared to pure randomness\n",
    "                min_length=0, \n",
    "                max_length=2)\n",
    "results = list(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(results):\n",
    "    item_sets = []\n",
    "    supports = [] \n",
    "    lhs = []\n",
    "    rhs = []\n",
    "    confidences = []\n",
    "    lifts = []\n",
    "    for result in results:\n",
    "        for subset in result[2]:\n",
    "            item_sets.append(tuple(result[0]))\n",
    "            supports.append(result[1])\n",
    "            lhs.append(tuple(subset[0]))\n",
    "            rhs.append(tuple(subset[1]))\n",
    "            confidences.append(subset[2])\n",
    "            lifts.append(subset[3])\n",
    "    return list(zip(item_sets, lhs, rhs, supports, confidences, lifts))\n",
    "results_df = pd.DataFrame(inspect(results),\n",
    "                          columns = ['Item Set', 'Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eclat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/apyori/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising items' frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stack().value_counts(normalize=True)[:10].plot(kind='bar', title='Relative Frequency')\n",
    "df.stack().value_counts().apply(lambda item: item / df.shape[0])[:10].plot(kind='bar', title='Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the dataset (models's input is a list of lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = []\n",
    "for i in range(df.shape[0]):\n",
    "    row = df.iloc[i].dropna().tolist()\n",
    "    transactions.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install apyori\n",
    "from apyori import apriori\n",
    "rules = apriori(transactions=transactions, \n",
    "                min_support=0.2, # how frequent is your item(s) in the dataset\n",
    "                min_length=0, \n",
    "                max_length=2)\n",
    "results = list(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(results):\n",
    "    item_sets = []\n",
    "    supports = [] \n",
    "    for result in results:\n",
    "        item_sets.append(tuple(result[0]))\n",
    "        supports.append(result[1])\n",
    "    return list(zip(item_sets, supports))\n",
    "results_df = pd.DataFrame(inspect(results), \n",
    "                          columns = ['Item Set', 'Support'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using Linear SVM, Kernel SVM and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def text_process(document):\n",
    "    document = re.sub('[^a-zA-Z]', ' ', document)\n",
    "    document = document.lower()\n",
    "    document = document.split()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    document = [word for word in document if not word in set(all_stopwords)]\n",
    "    ps = PorterStemmer()\n",
    "    document = [ps.stem(word) for word in document]\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_SVM_param_grid = {\n",
    "    'bag_of_words__ngram_range': [(1, 1), (1, 2), (1 ,3), (1, 4), (2, 2)],\n",
    "    'bag_of_words__max_df': [0.25, 0.5, 0.75, 0.85, 1.0],\n",
    "    'bag_of_words__min_df': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'bag_of_words__binary': [True, False],\n",
    "    'estimator__kernel': ['linear'],\n",
    "    'estimator__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "kernel_SVM_param_grid = {\n",
    "    'bag_of_words__ngram_range': [(1, 1), (1, 2), (1 ,3), (1, 4), (2, 2)],\n",
    "    'bag_of_words__max_df': [0.25, 0.5, 0.75, 0.85, 1.0],\n",
    "    'bag_of_words__min_df': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'bag_of_words__binary': [True, False],\n",
    "    'estimator__kernel': ['rbf'],\n",
    "    'estimator__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'estimator__gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "naive_bayes_SVM_param_grid = {\n",
    "    'bag_of_words__ngram_range': [(1, 1), (1, 2), (1 ,3), (1, 4), (2, 2)],\n",
    "    'bag_of_words__max_df': [0.25, 0.5, 0.75, 0.85, 1.0],\n",
    "    'bag_of_words__min_df': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    'bag_of_words__binary': [True, False],\n",
    "    'estimator__alpha': [0.001, 0.01, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_SVM_param_grid = {\n",
    "    'bag_of_words__ngram_range': [(1, 1), (1, 2)],\n",
    "    'bag_of_words__max_df': [0.85, 1.0],\n",
    "    'bag_of_words__min_df': [0.01, 0.05],\n",
    "    'estimator__kernel': ['linear'],\n",
    "    'estimator__C': [0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "kernel_SVM_param_grid = {\n",
    "    'bag_of_words__ngram_range': [(1, 1), (1, 2)],\n",
    "    'bag_of_words__max_df': [0.85, 1.0],\n",
    "    'bag_of_words__min_df': [0.01, 0.05],\n",
    "    'estimator__kernel': ['linear'],\n",
    "    'estimator__kernel': ['rbf'],\n",
    "    'estimator__C': [0.1, 1, 10, 100, 1000],\n",
    "    'estimator__gamma': [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "nb_param_grid = {\n",
    "    'bag_of_words__ngram_range': [(1, 1), (1, 2)],\n",
    "    'bag_of_words__max_df': [0.85, 1.0],\n",
    "    'bag_of_words__min_df': [0.01, 0.05],\n",
    "    'estimator__kernel': ['linear'],\n",
    "    'estimator__alpha': [0.001, 0.01, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "SVM_pipe = Pipeline([\n",
    "    ('bag_of_words', CountVectorizer(analyzer=text_process)),\n",
    "    ('tf_idf', TfidfTransformer()),\n",
    "    ('estimator', SVC())\n",
    "])\n",
    "\n",
    "nb_pipe = Pipeline([\n",
    "    ('bag_of_words', CountVectorizer(analyzer=text_process)),\n",
    "    ('tf_idf', TfidfTransformer()),\n",
    "    ('estimator', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "linear_SVM_grid = GridSearchCV(SVM_pipe, linear_SVM_param_grid, verbose=2, cv=2)\n",
    "linear_SVM_grid.fit(X_train, y_train)\n",
    "\n",
    "kernel_SVM_grid = GridSearchCV(SVM_pipe, kernel_SVM_param_grid, verbose=2, cv=2)\n",
    "kernel_SVM_grid.fit(X_train, y_train)\n",
    "\n",
    "nb_grid = GridSearchCV(nb_pipe, nb_param_grid, verbose=2, cv=2)\n",
    "nb_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting attributes (if using Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([\n",
    "    ('bag_of_words', CountVectorizer(analyzer=text_process, best_params_)), # best_params given by rf_grid.best_params_\n",
    "    ('tf_idf', TfidfTransformer()),\n",
    "    ('estimator', RandomForestClassifier(best_params)) # best_params given by rf_grid.best_params_\n",
    "])\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "feature_importance = pd.DataFrame(rf_pipe.steps[2][1].feature_importances_, \n",
    "                                  rf_pipe.steps[0][1].get_feature_names(), \n",
    "                                  columns=['importance'])\n",
    "feature_importance.sort_values('importance', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = df.user_id_col_name.nunique()\n",
    "n_items = df.item_id_col_name.nunique()\n",
    "\n",
    "train_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in train_data.itertuples():\n",
    "    # The \"-1\" is used if the user_id and/or item_id starts at 1\n",
    "    train_data_matrix[line[user_col_position]-1, line[item_col_position]-1] = line[rating_col_index]\n",
    "\n",
    "test_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in test_data.itertuples():\n",
    "    # The -1 is used if the user_id and/or item_id starts at 1\n",
    "    test_data_matrix[line[user_col_position]-1, line[item_col_position]-1] = line[rating_col_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Memotry-Based User-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cosine similarity:\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://latex.codecogs.com/gif.latex?s_u^{cos}(u_k,u_a)=\\frac{u_k&space;\\cdot&space;u_a&space;}{&space;\\left&space;\\|&space;u_k&space;\\right&space;\\|&space;\\left&space;\\|&space;u_a&space;\\right&space;\\|&space;}&space;=\\frac{\\sum&space;x_{k,m}x_{a,m}}{\\sqrt{\\sum&space;x_{k,m}^2\\sum&space;x_{a,m}^2}}\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "user_similarity = pairwise_distances(train_data_matrix, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-based:\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://latex.codecogs.com/gif.latex?\\hat{x}_{k,m}&space;=&space;\\bar{x}_{k}&space;&plus;&space;\\frac{\\sum\\limits_{u_a}&space;sim_u(u_k,&space;u_a)&space;(x_{a,m}&space;-&space;\\bar{x_{u_a}})}{\\sum\\limits_{u_a}|sim_u(u_k,&space;u_a)|}\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user(ratings, similarity):\n",
    "    mean_user_rating = ratings.mean(axis=1)\n",
    "    ratings_diff = (ratings - mean_user_rating[:, np.newaxis]) \n",
    "    pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    return pred\n",
    "user_prediction = predict_user(train_data_matrix, user_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(prediction, ground_truth))\n",
    "print('Collaborative Filtering Memory-Based User-Based RMSE: ' + str(rmse(user_prediction, test_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Memory-Based Item-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cosine similarity:\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://latex.codecogs.com/gif.latex?s_u^{cos}(i_m,i_b)=\\frac{i_m&space;\\cdot&space;i_b&space;}{&space;\\left&space;\\|&space;i_m&space;\\right&space;\\|&space;\\left&space;\\|&space;i_b&space;\\right&space;\\|&space;}&space;=\\frac{\\sum&space;x_{a,m}x_{a,b}}{\\sqrt{\\sum&space;x_{a,m}^2\\sum&space;x_{a,b}^2}}\n",
    "\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item-based:\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://latex.codecogs.com/gif.latex?\\hat{x}_{k,m}&space;=&space;\\frac{\\sum\\limits_{i_b}&space;sim_i(i_m,&space;i_b)&space;(x_{k,b})&space;}{\\sum\\limits_{i_b}|sim_i(i_m,&space;i_b)|}\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_item(ratings, similarity):\n",
    "    pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])    \n",
    "    return pred\n",
    "item_prediction = predict_item(train_data_matrix, item_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(prediction, ground_truth\n",
    "print('Collaborative Filtering Memory-Based Item-Based RMSE: ' + str(rmse(item_prediction, test_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Model-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the machine learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Singular Value Decomposition:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?X=USV^T\" title=\"X=USV^T\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Choose k\n",
    "u, s, vt = svds(train_data_matrix, k = 20)\n",
    "s_diag_matrix=np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dot product of *`U`*, *`S`* and *`V^T`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User-based CF MSE: ' + str(rmse(X_pred, test_data_matrix)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
