{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data.csv')\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:,:])\n",
    "X[:,:] = imputer.transform(X[:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the independent variables using get_dummies (considering multicollinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df['cat_col'], drop_first=True)\n",
    "df = pd.concat([df.drop('cat_col', axis=1), dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the independent variables using OneHotEncoder (considering multicollinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_prev (l_in):\n",
    "    l_out = []\n",
    "    l_out.append(l_in[0])\n",
    "    for i in range(len(l_in)-1):\n",
    "        l_out.append(l_out[i] + l_in[i+1])\n",
    "    return [e - 1 for e in l_out]\n",
    "\n",
    "# df and X must have the same data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "columns_to_encode = [0, 2, 3] # Change here\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), columns_to_encode)], remainder='passthrough')\n",
    "columns_to_encode = [df.iloc[:, del_idx].nunique() for del_idx in columns_to_encode]\n",
    "columns_to_encode = sum_prev(columns_to_encode)\n",
    "X = np.array(ct.fit_transform(X))\n",
    "X = np.delete(X, columns_to_encode, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the independent variables using OneHotEncoder (not considering multicollinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [column_index_to_encode])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling and undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "sm = SMOTE()\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use when data is normally distributed\n",
    "\n",
    "Sensitive to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Max Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use when data is not normally distributed, or you have well defined value boundaries\n",
    "\n",
    "Sensitive to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sensitive to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning to original value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(X_train)\n",
    "scaler.inverse_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning using grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'param_1': [arg_1, arg_2], 'param_2': [arg_1, arg_2]}\n",
    "grid = GridSearchCV(ml_model(), param_grid, n_jobs=-1, cv=num_for_k_folds, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "y_pred = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid = {'param_1': [arg_1, arg_2], 'param_2': [arg_1, arg_2]}\n",
    "random_grid = RandomizedSearchCV(ml_model(), param_grid, n_iter=100, n_jobs=-1, cv=num_for_k_folds, verbose=1)\n",
    "random_grid.fit(X_train, y_train)\n",
    "print(random_grid.best_params_)\n",
    "print(random_grid.best_score_)\n",
    "y_pred = random_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict([[x1_value, x2_value]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing predicted values and actual values side-by-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using the test set (with feature scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ss_y.inverse_transform(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a value (with feature scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(ss_X.transform([[x1_value, x2_value]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing predicted values and actual values side-by-side (with feature scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = ss_y.inverse_transform(y_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics: https://scikit-learn.org/stable/modules/classes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.r2_score(y_test, y_pred))\n",
    "print(metrics.mean_absolute_error(y_test, y_pred))\n",
    "print(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot((y_test-y_pred)) # If it's a correct model choice, it should be normally distributed\n",
    "plt.xlabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the actual and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_test, y=y_pred) # If the model fitted well, it should be a straight line\n",
    "plt.xlabel('y_test')\n",
    "plt.ylabel('y_pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n",
    "print('Confusion matrix:\\n', metrics.confusion_matrix(y_test, y_pred))\n",
    "print('Accuracy:', np.round(metrics.accuracy_score(y_test, y_pred), 4))\n",
    "print('Precision:', np.round(metrics.precision_score(y_test, y_pred), 4))\n",
    "print('Recall:', np.round(metrics.recall_score(y_test, y_pred), 4))\n",
    "print('F1-Score:', np.round(metrics.f1_score(y_test, y_pred), 4))\n",
    "print(metrics.classification_report(y_test, y_pred)) # Better for multiclass problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the test set results (lower resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test, y_test, color='red') # Use train for training set\n",
    "plt.plot(X_train, regression.predict(X_train), color='blue')\n",
    "plt.title('y vs x (Test set)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the test set results (higher resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid_train = np.arange(min(X), max(X), 0.1)\n",
    "X_grid_train = X_grid_train.reshape(len(X_grid), 1)\n",
    "plt.scatter(ss_X.inverse_transform(X_test), ss_y.inverse_transform(y_test), color='red') # Use train for training set\n",
    "plt.plot(ss_X.inverse_transform(X_grid_train), ss_y.inverse_transfor(regression.predict(X_grid_train)), color='blue')\n",
    "plt.title('y vs x (Test set)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the test set results (lower resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = X_test, y_test # Use train for training set\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('y vs x (Test set)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the test set results (higher resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "X_set, y_set = ss_X.inverse_transform(X_test), y_test # Use train for training set\n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n",
    "                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\n",
    "plt.contourf(X1, X2, classifier.predict(ss_X.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n",
    "             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i, j in enumerate(np.unique(y_set)):\n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                c = ListedColormap(('red', 'green'))(i), label = j)\n",
    "plt.title('y vs x (Test set)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regression.coef_)\n",
    "print(regression.intercept_)\n",
    "pd.DataFrame(regressor.coef_, X_train.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regression.coef_)\n",
    "print(regression.intercept_)\n",
    "pd.DataFrame(regression.coef_, df.columns[:-1], columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Linear Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree = number_of_polynomials)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "regression = LinearRegression()\n",
    "regression.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regression.predict(poly_features.fit_transform([[x1_value, x2_value]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting linear equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regression.coef_)\n",
    "print(regression.intercept_)\n",
    "pd.DataFrame(regression.coef_, X_train.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the test set results (higher resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid_train = np.arange(min(X), max(X), 0.1)\n",
    "X_grid_train = X_grid_train.reshape(len(X_grid_train), 1)\n",
    "plt.scatter(X_test, y_test, color='red')\n",
    "plt.plot(X_grid_train, regression.predict(poly_reg.fit_transform(X_grid_train)), color='blue')\n",
    "plt.title('y vs x (Test set)')\n",
    "plt.ylabel('y')\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score \n",
    "degrees = [2, 3, 4, 5, 6] # Change degree \"hyperparameter\" here\n",
    "normalizes = [True, False] # Change normalize hyperparameter here\n",
    "best_score = 0\n",
    "best_degree = 0\n",
    "for degree in degrees:\n",
    "    for normalize in normalizes:\n",
    "        poly_features = PolynomialFeatures(degree = degree)\n",
    "        X_train_poly = poly_features.fit_transform(X_train)\n",
    "        polynomial_regression = LinearRegression(normalize=normalize)\n",
    "        polynomial_regression.fit(X_train_poly, y_train)\n",
    "        scores = cross_val_score(polynomial_regression, X_train_poly, y_train, cv=5) # Change k-fold cv value here\n",
    "        if max(scores) > best_score:\n",
    "            best_score = max(scores)\n",
    "            best_degree = degree\n",
    "            best_normalize = normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "\n",
    "Needs feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "regressor = SVR(kernel='kernal_name')\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100, 1000], \n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100],\n",
    "     'kernel': ['rbf', 'sigmoid']},\n",
    "    {'C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100, 1000],\n",
    "     'kernel': ['linear']},\n",
    "    {'C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100, 1000],\n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100],\n",
    "     'degree': [2, 3, 4, 5, 6],\n",
    "     'kernel': ['poly']}\n",
    "]\n",
    "grid = GridSearchCV(SVR(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'C': [0.1, 0.5, 1, 5, 10, 100, 1000], \n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 0.5, 1],\n",
    "     'kernel': ['rbf']},\n",
    "    {'C': [0.1, 0.5, 1, 5, 10, 100, 1000],\n",
    "     'kernel': ['linear']},\n",
    "    {'C': [0.1, 0.5, 1, 5, 10, 100, 1000],\n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5],\n",
    "     'degree': [2, 3],\n",
    "     'kernel': ['poly']}\n",
    "]\n",
    "grid = GridSearchCV(SVR(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Non-continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor()\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['mse', 'friedman_mse', 'mae'],\n",
    "    'max_depth': [None, 5, 10, 20, 30, 40, 50, 80, 90, 100, 110],\n",
    "    'max_features': [2, 3, 5, 10, 'auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 10, 15, 100],\n",
    "    'min_samples_split': [2, 5, 8, 10, 12, 15, 20],\n",
    "    'n_estimators': [100, 200, 300, 500, 800, 1000, 1500, 2500]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestRegressor(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [10, 40, 70, 100],\n",
    "    'max_features': [2, 3, 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 3, 6, 10, 14],\n",
    "    'min_samples_split': [2, 6, 10, 14],\n",
    "    'n_estimators': [100, 300]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestRegressor(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Non-continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators=200)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [None, 5, 10, 20, 30, 40, 50, 80, 90, 100, 110],\n",
    "    'max_features': [2, 3, 5, 10, 'auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 10, 15, 100],\n",
    "    'min_samples_split': [2, 5, 8, 10, 12, 15, 20],\n",
    "    'n_estimators': [100, 200, 300, 500, 800, 1000, 1500, 2500]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestRegressor(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [10, 40, 70, 100],\n",
    "    'max_features': [2, 3, 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 3, 6, 10, 14],\n",
    "    'min_samples_split': [2, 6, 10, 14],\n",
    "    'n_estimators': [100, 300]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestRegressor(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Non-continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "classifier = XGBRegressor()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimator': [20, 60, 100, 140, 180, 220, 500, 1000],\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'learning_rate': [0.0005, 0.1, 0.01, 0.05, 0.1, 0.5, 1],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 1, 1.5, 2, 5],\n",
    "    'min_child_weight': [0.01, 0.1, 1, 3, 4, 5, 6, 8, 10, 12],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [00000.1, 0.01, 0.1, 1, 100]\n",
    "}\n",
    "grid = GridSearchCV(XGBRegressor(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimator': [20, 100, 250, 500, 1000],\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'gamma': [0, 0.5, 1],\n",
    "    'min_child_weight': [6, 10],\n",
    "    'subsample': [0.6, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.6, 0.8]\n",
    "}\n",
    "grid = GridSearchCV(XGBRegressor(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Needs feature scaling\n",
    "\n",
    "Linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'penalty': ['none', 'l1', 'l2', 'elasticnet'], \n",
    "    'C': list(np.logspace(0, 4, 10)) + [0.0001, 0.01, 0.05, 0.2, 10, 1000],\n",
    "    'solver': [\"newton-cg\", \"lbfgs\", \"liblinear\", 'sag', 'saga'],\n",
    "    'dual': [True, False]\n",
    "}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'], \n",
    "    'C': [0.001, 0.01, 1, 10, 100],\n",
    "    'solver': [\"newton-cg\", \"lbfgs\"]\n",
    "}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "    \n",
    "Needs feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method to find optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "error_rate = []\n",
    "\n",
    "for i in range(1, 40, 2):\n",
    "    knn_elbow = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_elbow.fit(X_train, y_train)\n",
    "    y_pred = knn_elbow.predict(X_test)\n",
    "    error_rate.append(1 - accuracy_score(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1, 40, 2), error_rate, color='blue', ls='--', marker='o', markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'n_neighbors': list(range(1, 31, 2)),\n",
    "     'weights': ['uniform', 'distance'],\n",
    "     'metric': ['euclidean', 'manhattan', 'minkowski']},\n",
    "    {'n_neighbors': list(range(1, 31, 2)),\n",
    "     'weights': ['uniform', 'distance'],\n",
    "     'p': [3, 4, 5]\n",
    "     'metric': ['minkowski']}\n",
    "]\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 25, 2)),\n",
    "    'p': [1, 2],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    \n",
    "Needs feature scaling\n",
    "\n",
    "Non-linear (unless if using the linear kernel)\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# The larger the C value the more the model will overfit\n",
    "# The larger the gamma value the more the model will underfit\n",
    "classifier = SVC(kernel='rbf')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100, 1000], \n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100],\n",
    "     'kernel': ['rbf', 'sigmoid']},\n",
    "    {'C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100, 1000],\n",
    "     'kernel': ['linear']},\n",
    "    {'C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100, 1000],\n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 100],\n",
    "     'degree': [2, 3, 4, 5, 6],\n",
    "     'kernel': ['poly']}\n",
    "]\n",
    "grid = GridSearchCV(SVC(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'C': [0.1, 0.5, 1, 5, 10, 100, 1000], \n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 0.5, 1],\n",
    "     'kernel': ['rbf']},\n",
    "    {'C': [0.1, 0.5, 1, 5, 10, 100, 1000],\n",
    "     'kernel': ['linear']},\n",
    "    {'C': [0.1, 0.5, 1, 5, 10, 100, 1000],\n",
    "     'gamma': [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5],\n",
    "     'degree': [2, 3],\n",
    "     'kernel': ['poly']}\n",
    "]\n",
    "grid = GridSearchCV(SVC(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Non-continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10, 20, 30, 40, 50, 80, 90, 100, 110],\n",
    "    'max_features': [2, 3, 5, 10, 'auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 10, 15, 100],\n",
    "    'min_samples_split': [2, 5, 8, 10, 12, 15, 20],\n",
    "    'n_estimators': [100, 200, 300, 500, 800, 1000, 1500, 2500]\n",
    "}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [10, 40, 70, 100],\n",
    "    'max_features': [2, 3, 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 3, 6, 10, 14],\n",
    "    'min_samples_split': [2, 6, 10, 14],\n",
    "    'n_estimators': [100, 300]\n",
    "}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Non-continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regressor = RandomForestClassifier(n_estimators=200)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [None, 5, 10, 20, 30, 40, 50, 80, 90, 100, 110],\n",
    "    'max_features': [2, 3, 5, 10, 'auto', 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5, 10, 15, 100],\n",
    "    'min_samples_split': [2, 5, 8, 10, 12, 15, 20],\n",
    "    'n_estimators': [100, 200, 300, 500, 800, 1000, 1500, 2500]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [10, 40, 70, 100],\n",
    "    'max_features': [2, 3, 'sqrt', 'log2'],\n",
    "    'min_samples_leaf': [1, 3, 6, 10, 14],\n",
    "    'min_samples_split': [2, 6, 10, 14],\n",
    "    'n_estimators': [100, 300]\n",
    "}\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Non-continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'learning_rate': [0.0005, 0.1, 0.01, 0.05, 0.1, 0.5, 1],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 1, 1.5, 2, 5],\n",
    "    'min_child_weight': [0.01, 0.1, 1, 3, 4, 5, 6, 8, 10, 12],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [00000.1, 0.01, 0.1, 1, 100],\n",
    "}\n",
    "grid = GridSearchCV(XGBClassifier(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'gamma': [0, 0.5, 1],\n",
    "    'min_child_weight': [6, 10],\n",
    "    'subsample': [0.6, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.6, 0.8],\n",
    "}\n",
    "grid = GridSearchCV(XGBClassifier(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNBm\n",
    "\n",
    "No need for feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 1.0]\n",
    "}\n",
    "grid = GridSearchCV(MultinomialNB(), param_grid, n_jobs=-1, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "Needs feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method to find optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "\n",
    "for k in range(2, 21):\n",
    "    kmeans_elbow = KMeans(n_clusters=k)\n",
    "    kmeans_elbow.fit(X)\n",
    "    wcss.append(kmeans_elbow.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(2, 21), wcss, ls='--', marker='o', markerfacecolor='red', markersize=10)\n",
    "plt.title('WCSS vs k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('WCSS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=8)\n",
    "kmeans.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'blue', 'green', 'cyan', 'magenta']\n",
    "\n",
    "for i in range(kmeans.n_clusters):\n",
    "    plt.scatter(X[kmeans.labels_ == i, 0], X[kmeans.labels_ == i, 1], c=colors[i], s=50, label='Cluster ' + str(i))\n",
    "\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=150, c='black', label='Centroids')  \n",
    "plt.title('Clusters')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering\n",
    "\n",
    "Needs feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendogram to find optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "dendogram = sch.dendrogram(sch.linkage(X, metric='euclidean', method='ward'), no_labels=True)\n",
    "plt.title('Dendogram')\n",
    "plt.xlabel('Rows')\n",
    "plt.ylabel('Euclidean distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "y_pred = hc.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'blue', 'green', 'cyan', 'magenta']\n",
    "\n",
    "for i in range(hc.n_clusters):\n",
    "    plt.scatter(X[hc.labels_ == i, 0], X[hc.labels_ == i, 1], c=colors[i], s=50, label='Cluster ' + str(i))\n",
    "\n",
    "plt.title('Clusters')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "N = df.shape[0]\n",
    "d = df.shape[1]\n",
    "options_selected = []\n",
    "numbers_of_selections = [1] * d\n",
    "sums_of_rewards = [0] * d\n",
    "total_reward = 0\n",
    "\n",
    "for n in range(N):\n",
    "    option = 0\n",
    "    max_upper_bound = 0\n",
    "    for i in range(d):\n",
    "        average_reward = sums_of_rewards[i] / numbers_of_selections[i]\n",
    "        delta_i = math.sqrt(3/2 * math.log(n+1) / numbers_of_selections[i])\n",
    "        upper_bound = average_reward + delta_i\n",
    "        if upper_bound > max_upper_bound:\n",
    "            max_upper_bound = upper_bound\n",
    "            option = i\n",
    "    options_selected.append(option)\n",
    "    numbers_of_selections[option] += 1\n",
    "    reward = df.iloc[n, option]\n",
    "    sums_of_rewards[option] += reward\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the options selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(options_selected)\n",
    "plt.title('Histogram of options selections')\n",
    "plt.xlabel('Options')\n",
    "plt.ylabel('Number of times each option was selected by UCB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N = df.shape[0]\n",
    "d = df.shape[1]\n",
    "options_selected = []\n",
    "number_of_rewards_1 = [0] * d\n",
    "number_of_rewards_0 = [0] * d\n",
    "total_reward = 0\n",
    "\n",
    "for n in range(N):\n",
    "    option = 0\n",
    "    max_random_beta = 0\n",
    "    for i in range(d):\n",
    "        random_beta = random.betavariate(number_of_rewards_1[i] + 1, number_of_rewards_0[i] + 1)\n",
    "        if random_beta > max_random_beta:\n",
    "            max_random_beta = random_beta\n",
    "            option = i\n",
    "    options_selected.append(option)\n",
    "    reward = df.iloc[n, option]\n",
    "    if reward == 1:\n",
    "        number_of_rewards_1[option] += 1\n",
    "    else:\n",
    "        number_of_rewards_0[option] += 1\n",
    "    total_reward += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the options selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(options_selected)\n",
    "plt.title('Histogram of ads selections')\n",
    "plt.xlabel('Ads')\n",
    "plt.ylabel('Number of times each ad was selected by the algorithm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rule Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/apyori/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising items' frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stack().value_counts(normalize=True)[:10].plot(kind='bar', title='Relative Frequency')\n",
    "df.stack().value_counts().apply(lambda item: item / df.shape[0])[:10].plot(kind='bar', title='Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the dataset (models's input is a list of lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = []\n",
    "for i in range(df.shape[0]):\n",
    "    row = df.iloc[i].dropna().tolist()\n",
    "    transactions.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "rules = apriori(transactions=transactions, \n",
    "                min_support=0.2, # how frequent is your item(s) in the dataset \n",
    "                min_confidence=0.6, # how often your rule will work\n",
    "                min_lift=2, # how better off you are compared to pure randomness\n",
    "                min_length=0, \n",
    "                max_length=2)\n",
    "results = list(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(results):\n",
    "    item_sets = []\n",
    "    supports = [] \n",
    "    lhs = []\n",
    "    rhs = []\n",
    "    confidences = []\n",
    "    lifts = []\n",
    "    for result in results:\n",
    "        for subset in result[2]:\n",
    "            item_sets.append(tuple(result[0]))\n",
    "            supports.append(result[1])\n",
    "            lhs.append(tuple(subset[0]))\n",
    "            rhs.append(tuple(subset[1]))\n",
    "            confidences.append(subset[2])\n",
    "            lifts.append(subset[3])\n",
    "    return list(zip(item_sets, lhs, rhs, supports, confidences, lifts))\n",
    "results_df = pd.DataFrame(inspect(results),\n",
    "                          columns = ['Item Set', 'Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eclat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/apyori/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising items' frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stack().value_counts(normalize=True)[:10].plot(kind='bar', title='Relative Frequency')\n",
    "df.stack().value_counts().apply(lambda item: item / df.shape[0])[:10].plot(kind='bar', title='Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the dataset (models's input is a list of lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = []\n",
    "for i in range(df.shape[0]):\n",
    "    row = df.iloc[i].dropna().tolist()\n",
    "    transactions.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "rules = apriori(transactions=transactions, \n",
    "                min_support=0.2, # how frequent is your item(s) in the dataset\n",
    "                min_length=0, \n",
    "                max_length=2)\n",
    "results = list(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(results):\n",
    "    item_sets = []\n",
    "    supports = [] \n",
    "    for result in results:\n",
    "        item_sets.append(tuple(result[0]))\n",
    "        supports.append(result[1])\n",
    "    return list(zip(item_sets, supports))\n",
    "results_df = pd.DataFrame(inspect(results), \n",
    "                          columns = ['Item Set', 'Support'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nltk.org/py-modindex.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def text_process(document):\n",
    "    document = re.sub('[^a-zA-Z]', ' ', document)\n",
    "    document = document.lower()\n",
    "    document = document.split()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    document = [word for word in document if not word in set(all_stopwords)]\n",
    "    ps = PorterStemmer()\n",
    "    document = [ps.stem(word) for word in document]\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('bag_of_words', CountVectorizer(analyzer=text_process)),\n",
    "    ('tf_idf', TfidfTransformer()),\n",
    "    ('estimator', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (more in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_param = [\n",
    "    {'bag_of_words__ngram_range': [(1, 1), (1, 2), (1 ,3), (1, 4), (2, 2)],\n",
    "     'bag_of_words__max_df': [0.25, 0.5, 0.75, 0.85, 1.0],\n",
    "     'bag_of_words__min_df': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "     'bag_of_words__binary': [True, False],\n",
    "     'estimator': [SVC()]\n",
    "     'estimator__kernel': ['linear'],\n",
    "     'estimator__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
    "    {'bag_of_words__ngram_range': [(1, 1), (1, 2), (1 ,3), (1, 4), (2, 2)],\n",
    "     'bag_of_words__max_df': [0.25, 0.5, 0.75, 0.85, 1.0],\n",
    "     'bag_of_words__min_df': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "     'bag_of_words__binary': [True, False],\n",
    "     'estimator': [MultinomialNB()]\n",
    "     'estimator__alpha': [0.001, 0.01, 1.0]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning using grid search (less in-depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_param = [\n",
    "    {'bag_of_words__ngram_range': [(1, 1), (1, 2)],\n",
    "     'bag_of_words__max_df': [0.85, 1.0],\n",
    "     'bag_of_words__min_df': [0.01, 0.05],\n",
    "     'estimator': [SVC()]\n",
    "     'estimator__kernel': ['linear'],\n",
    "     'estimator__C': [0.1, 1, 10, 100, 1000]},\n",
    "    {'bag_of_words__ngram_range': [(1, 1), (1, 2)],\n",
    "     'bag_of_words__max_df': [0.85, 1.0],\n",
    "     'bag_of_words__min_df': [0.01, 0.05],\n",
    "     'estimator': [MultinomialNB()]\n",
    "     'estimator__alpha': [0.001, 0.01, 1.0]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(pipe, param_grid, n_jobs=-1, cv=5, verbose=1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting attributes (if using Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline([\n",
    "    ('bag_of_words', CountVectorizer(analyzer=text_process, best_params_)), # best_params given by rf_grid.best_params_\n",
    "    ('tf_idf', TfidfTransformer()),\n",
    "    ('estimator', RandomForestClassifier(best_params)) # best_params given by rf_grid.best_params_\n",
    "])\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "feature_importance = pd.DataFrame(rf_pipe.steps[2][1].feature_importances_, \n",
    "                                  rf_pipe.steps[0][1].get_feature_names(), \n",
    "                                  columns=['importance'])\n",
    "feature_importance.sort_values('importance', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = df.user_id_col_name.nunique()\n",
    "n_items = df.item_id_col_name.nunique()\n",
    "\n",
    "train_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in train_data.itertuples():\n",
    "    # The \"-1\" is used if the user_id and/or item_id starts at 1\n",
    "    train_data_matrix[line[user_col_position]-1, line[item_col_position]-1] = line[rating_col_index]\n",
    "\n",
    "test_data_matrix = np.zeros((n_users, n_items))\n",
    "for line in test_data.itertuples():\n",
    "    # The -1 is used if the user_id and/or item_id starts at 1\n",
    "    test_data_matrix[line[user_col_position]-1, line[item_col_position]-1] = line[rating_col_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Memory-Based User-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cosine similarity:\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://latex.codecogs.com/gif.latex?s_u^{cos}(u_k,u_a)=\\frac{u_k&space;\\cdot&space;u_a&space;}{&space;\\left&space;\\|&space;u_k&space;\\right&space;\\|&space;\\left&space;\\|&space;u_a&space;\\right&space;\\|&space;}&space;=\\frac{\\sum&space;x_{k,m}x_{a,m}}{\\sqrt{\\sum&space;x_{k,m}^2\\sum&space;x_{a,m}^2}}\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "user_similarity = pairwise_distances(train_data_matrix, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-based:\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://latex.codecogs.com/gif.latex?\\hat{x}_{k,m}&space;=&space;\\bar{x}_{k}&space;&plus;&space;\\frac{\\sum\\limits_{u_a}&space;sim_u(u_k,&space;u_a)&space;(x_{a,m}&space;-&space;\\bar{x_{u_a}})}{\\sum\\limits_{u_a}|sim_u(u_k,&space;u_a)|}\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user(ratings, similarity):\n",
    "    mean_user_rating = ratings.mean(axis=1)\n",
    "    ratings_diff = (ratings - mean_user_rating[:, np.newaxis]) \n",
    "    pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    return pred\n",
    "user_prediction = predict_user(train_data_matrix, user_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(prediction, ground_truth))\n",
    "print('Collaborative Filtering Memory-Based User-Based RMSE: ' + str(rmse(user_prediction, test_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Memory-Based Item-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cosine similarity:\n",
    "\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://latex.codecogs.com/gif.latex?s_u^{cos}(i_m,i_b)=\\frac{i_m&space;\\cdot&space;i_b&space;}{&space;\\left&space;\\|&space;i_m&space;\\right&space;\\|&space;\\left&space;\\|&space;i_b&space;\\right&space;\\|&space;}&space;=\\frac{\\sum&space;x_{a,m}x_{a,b}}{\\sqrt{\\sum&space;x_{a,m}^2\\sum&space;x_{a,b}^2}}\n",
    "\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "item_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item-based:\n",
    "<img class=\"aligncenter size-thumbnail img-responsive\" src=\"https://latex.codecogs.com/gif.latex?\\hat{x}_{k,m}&space;=&space;\\frac{\\sum\\limits_{i_b}&space;sim_i(i_m,&space;i_b)&space;(x_{k,b})&space;}{\\sum\\limits_{i_b}|sim_i(i_m,&space;i_b)|}\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_item(ratings, similarity):\n",
    "    pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])    \n",
    "    return pred\n",
    "item_prediction = predict_item(train_data_matrix, item_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(prediction, ground_truth\n",
    "print('Collaborative Filtering Memory-Based Item-Based RMSE: ' + str(rmse(item_prediction, test_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Model-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the machine learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Singular Value Decomposition:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?X=USV^T\" title=\"X=USV^T\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Choose k\n",
    "u, s, vt = svds(train_data_matrix, k = 20)\n",
    "s_diag_matrix=np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dot product of *`U`*, *`S`* and *`V^T`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User-based CF MSE: ' + str(rmse(X_pred, test_data_matrix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/api_docs/python/tf/keras\n",
    "\n",
    "Needs feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating early stop condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "dff = Sequential()\n",
    "dff.add(Dense(units=X_train.shape[1], activation='relu'))\n",
    "dff.add(Dropout(0.2))\n",
    "dff.add(Dense(units=X_train.shape[1] // 2, activation='relu'))\n",
    "dff.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the output neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.add(Dense(n_classes, activation='softmax')) # Multiclass\n",
    "dff.add(Dense(1, activation='sigmoid')) # Binary\n",
    "dff.add(Dense(1, activation='linear')) # Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # Multiclass\n",
    "dff.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Binary\n",
    "dff.compile(optimizer='adam', loss='mse', metrics=['mae']) # Regression\n",
    "dff.fit(X, y, batch_size=32, epochs=1000, validation_data=(X_test, y_test), callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting loss and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(dff.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/api/preprocessing/image/\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras\n",
    "\n",
    "Needs feature scaling\n",
    "\n",
    "Non-linear\n",
    "\n",
    "Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling and image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, # Feature scaling\n",
    "    shear_range=0.2, # Image augmentation\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "    'data/training_set',\n",
    "     target_size=(64, 64),\n",
    "     batch_size=32,\n",
    "     class_mode='binary'\n",
    ")\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'data/test_set',\n",
    "    target_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))\n",
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the output neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(n_classes, activation='softmax')) # Multiclass\n",
    "cnn.add(Dense(1, activation='sigmoid')) # Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # Multiclass\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Binary\n",
    "cnn.fit(training_set, epochs=1000, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('data/single_prediction/image.jpg', target_size=(64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indeces\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'class 1'\n",
    "else:\n",
    "    prediction = 'class 2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html\n",
    "\n",
    "Feature selection, filter method\n",
    "\n",
    "Unsupervised\n",
    "\n",
    "Works for both continuous and one hot encoded features\n",
    "\n",
    "Needs feature scaling (but it can't be Standard Scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "cols = df.drop('y_col', axis=1).columns\n",
    "def variance_threshold_selector(X_train, threshold):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(X_train)\n",
    "    col_names = cols\n",
    "    col_bools = selector.get_support()\n",
    "    relevant_cols = [col_name for col_name, col_bool in zip(col_names, col_bools) if col_bool == True]\n",
    "    irrelevant_cols = [col_name for col_name, col_bool in zip(col_names, col_bools) if col_bool == False]\n",
    "    df_selected = df[relevant_cols]\n",
    "    df_removed = df[irrelevant_cols]\n",
    "    return (df_selected, df_removed)\n",
    "df_selected, df_removed = variance_threshold_selector(X_train, 0.01) # Change here \n",
    "# 0.0 to remove constant features, 0.01 to remove quasi-contant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_var = list(df_selected.columns)\n",
    "removed_features_var = list(df_removed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-Square Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html\n",
    "\n",
    "Feature selection, filter method\n",
    "\n",
    "Supervised\n",
    "\n",
    "Only works for one hot encoded features\n",
    "\n",
    "Only works for classification\n",
    "\n",
    "Needs feature scaling (but it can't be Standard Scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "chi_scores = chi2(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p_values = pd.DataFrame({'p-value': chi_scores[1]})\n",
    "df_p_values.index = df.drop('MEDV', axis=1).columns\n",
    "df_p_values.sort_values(by='p-value', ascending=False , inplace=True)\n",
    "df_p_values.plot.bar()\n",
    "df_p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 # Change here\n",
    "selected_features_chi = list(df_p_values[df_p_values['p-value'] <= alpha].index)\n",
    "removed_features_chi = list(df_p_values[df_p_values['p-value'] > alpha].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection, filter method\n",
    "\n",
    "Supervised / Unsupervised\n",
    "\n",
    "Works for both continuous and one hot encoded features\n",
    "\n",
    "Works for both regression and classification\n",
    "\n",
    "Needs feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising dataframe correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "cor = df.corr()\n",
    "sns.heatmap(cor, annot=True, cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting features with strong correlation with the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_y = abs(cor['y_col'])\n",
    "selected_features_cor = cor_y[cor_y > 0.5] # Change here\n",
    "df_selected = df[selected_features_cor.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising relevant dataframe correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "cor = df_selected.corr()\n",
    "sns.heatmap(cor, annot=True, cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting features with weak correlation with every other feature, keeping the one most correlated with the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_selected[['feat_1', 'feat_2', 'feat_3', 'feat_4']] # Decide on a new correlation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_cor = list(df_selected.columns)\n",
    "removed_features_cor = list(set(df.drop('y_col', axis=1).columns) - set(selected_features_cor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html\n",
    "\n",
    "Feature selection, wrapper method\n",
    "\n",
    "Supervised\n",
    "\n",
    "Works for both continuous and one hot encoded features\n",
    "\n",
    "Works for both regression and classification\n",
    "\n",
    "Needs feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "feat_cols = list(df.drop('y_col', axis=1).columns)\n",
    "pmax = 1\n",
    "while (len(feat_cols) > 0):\n",
    "    p = []\n",
    "    const_col = df[feat_cols]\n",
    "    const_col = sm.add_constant(const_col)\n",
    "    model = sm.OLS(y_train, const_col).fit()\n",
    "    p = pd.Series(model.pvalues[1:], index=feat_cols)\n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax > 0.05): # Change here\n",
    "        feat_cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_back = list(feat_cols)\n",
    "removed_features_back = list(set(df.drop('y_col', axis=1).columns) - set(feat_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    "\n",
    "Supervised\n",
    "\n",
    "Feature selection, wrapper method\n",
    "\n",
    "Works for both continuous and one hot encoded features\n",
    "\n",
    "Works for both regression and classification\n",
    "\n",
    "Feature scaling depends on the model used (usually not required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe to find optimal number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from sklearn.svm import SVR # Has to use linear kernel\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "high_score = 0\n",
    "optimal_n = 0\n",
    "ns = np.arange(1, df.drop('y_col', axis=1).shape[1] + 1)\n",
    "score_list = []\n",
    "for n in ns:\n",
    "    X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n",
    "    model = LinearRegression() # Change here\n",
    "    rfe = RFE(model, n_features_to_select = n)\n",
    "    X_train_rfe = rfe.fit_transform(X_train_temp, y_train_temp)\n",
    "    X_test_rfe = rfe.transform(X_test_temp)\n",
    "    model.fit(X_train_rfe, y_train_temp)\n",
    "    score = model.score(X_test_rfe, y_test_temp)\n",
    "    score_list.append(score)\n",
    "    if(score > high_score):\n",
    "        high_score = score\n",
    "        optimal_n = n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Possible optimum number of features: %d\" %optimal_n)\n",
    "print(\"Score with %d features: %f\" %(optimal_n, high_score))\n",
    "df_selected = pd.DataFrame({'Score': score_list})\n",
    "df_selected.index = range(1, len(df_selected) + 1) # Number of Features\n",
    "df_selected.sort_values(by='Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting using the optimal number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.drop('y_col', axis=1))\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, n_features_to_select=optimal_n)      \n",
    "X_rfe = rfe.fit_transform(X_train, y_train)\n",
    "model.fit(X_rfe, y_train)              \n",
    "temp = pd.Series(rfe.support_, index = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_rfe = list(temp[temp == True].index)\n",
    "removed_features_rfe = list(temp[temp == False].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html\n",
    "\n",
    "Supervised\n",
    "\n",
    "Feature selection, embedded method\n",
    "\n",
    "Works for both continuous and one hot encoded features\n",
    "\n",
    "Works for both regression and classification\n",
    "\n",
    "Needs feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "reg = LassoCV()\n",
    "reg.fit(X_train, y_train)\n",
    "coef = pd.Series(reg.coef_, index = df.drop('y_col', axis=1).columns)\n",
    "coef = coef.sort_values()\n",
    "plt.figure(figsize=(8, 10))\n",
    "plt.title(\"Feature importance using Lasso Model\")\n",
    "coef.plot(kind = \"barh\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 10))\n",
    "plt.title(\"Feature importance using Lasso Model\")\n",
    "coef.plot(kind = \"barh\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_lasso = list(coef[coef != 0].index)\n",
    "removed_features_lasso = list(coef[coef == 0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all feature selections together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting selected features to true and removed features to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_false(selected_features, features):\n",
    "    selected_t_f = []\n",
    "    for feature in features:\n",
    "        if feature in selected_features:\n",
    "            selected_t_f.append(True)\n",
    "        else:\n",
    "            selected_t_f.append(False)\n",
    "    return selected_t_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe showing the most selected features at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sorted(df.drop('y_col', axis=1).columns)\n",
    "df_selections = pd.DataFrame({'Feature': features,\n",
    "                              'Variance': get_true_false(selected_features_var, features),\n",
    "                              'Chi-Squared': get_true_false(selected_features_chi, features),\n",
    "                              'Correlation': get_true_false(selected_features_cor, features),\n",
    "                              'Backward': get_true_false(selected_features_back, features),\n",
    "                              'RFE': get_true_false(selected_features_rfe, features),\n",
    "                              'Lasso': get_true_false(selected_features_lasso, features)})\n",
    "df_selections['Total'] = np.sum(df_selections, axis=1)\n",
    "df_selections = df_selections.sort_values(['Total', 'Feature'], ascending=False)\n",
    "df_selections.index = range(1, len(df_selections) + 1)\n",
    "df_selections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_selected = list(df_selections[df_selections['Total'] > votes]['Feature']) # Change votes threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "Feature extraction\n",
    "\n",
    "Unsupervised\n",
    "\n",
    "Only works for continuous features\n",
    "\n",
    "Needs feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot to find optimal number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.decomposition import KernelPCA # For KernelPCA\n",
    "pca_test = PCA()\n",
    "pca_test.fit(X_train)\n",
    "explained_variance_ratio = pca_test.explained_variance_ratio_ # Delete if using KernelPCA\n",
    "# explained_variance = np.var(kpca_test, axis=0) # For KernelPCA\n",
    "# explained_variance_ratio = explained_variance / np.sum(explained_variance) # For KernelPCA\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "plt.plot(np.cumsum(explained_variance_ratio))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "display(plt.show())\n",
    "\n",
    "evr = explained_variance_ratio\n",
    "cvr = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "pca_df = pd.DataFrame()\n",
    "pca_df['Cumulative Variance Ratio'] = cvr\n",
    "pca_df['Explained Variance Ratio'] = evr\n",
    "display(pca_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
    "\n",
    "Feature extraction\n",
    "\n",
    "Supervised\n",
    "\n",
    "Only works for continuous features\n",
    "\n",
    "No need for feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot to find optimal number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda_test = LDA(n_components=None)\n",
    "lda_test.fit(X_train, y_train)\n",
    "explained_variance_ratio = lda_test.explained_variance_ratio_\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "plt.plot(np.cumsum(explained_variance_ratio))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "display(plt.show())\n",
    "\n",
    "evr = explained_variance_ratio\n",
    "cvr = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "lda_df = pd.DataFrame()\n",
    "lda_df['Cumulative Variance Ratio'] = cvr\n",
    "lda_df['Explained Variance Ratio'] = evr\n",
    "display(lda_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components=2)\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
